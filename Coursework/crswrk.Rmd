---
title: "MTHM508 Coursework"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyr)
library(ggplot2)
library(tidyverse)
```

# Problem

Lucy has had 65 “yes” responses to baby Percy’s Birthday party invitation. She is planning the catering and there is a sickness bug going around that means it is likely a number of people will cancel.

Suppose that she judges that the probability of any given guest dropping out due to illness is 1 - $\theta$ (so probability of attending is $\theta$).

## We are making the tacit assumptions that, 
i. Out of the 65 people who have responded "yes", those who will drop out will do so only because of the sickness.
ii. Those who have been infected will not show up.

**1. Describe the exchangeability judgement that Lucy is making.**

*Definition 2.9.* A sequence ($Y_1, Y_2, . . .$) of random quantities is said to be
exchangeable if the joint probability distribution of each sub-collection of n
quantities ($Y_{i_1}, . . . , Y_{i_n}$) is the same.

i. **Comment**. To say that a set of random quantities is exchangeable is a
subjective judgement.

ii. **Comment**. Exchangeability amounts to indifference over re-labelling the random quantities or permuting them.

To simplify, for 2 r.v.s, $x_1$ and $x_2$, to be exchangeable, their joint probability distribution $P(x_1 , x_2) = P(x_2 , x_1)$. Therefore, *independent* and *identically distributed* random variables are exchangeable, by definition.

The event of someone attending the event can be modeled as a random variable, $X \sim Bern(\theta)$ with $0 \le \theta \le 1$. Thus, $P(X = x_i) = \theta^{x_i}(1-\theta)^{(1-x_i)}$. Here, Lucy views all x_i to be i.i.d., thereby allowing her to make the exchangeability judgement. In simple terms,  she judges that the probability of **any given guest** is $\theta$, i.e., she views the chance of one person attending the event as equal to that of any other person and thus, the joint probability does not change with any shuffling of the $x_i$'s.

**2. Suppose that you make the same exchangeability judgement as Lucy yourself.**
Derive and carefully justify your Beta prior distribution for $\theta$, $\pi(\theta)$.

a. Use the quantile method seen in lectures and fit your prior with the MATCH tool.

There may be a few ways to look at this - it depends on how infectious the bug is. With no further information, if we assume that the chance of being infected is about $\frac{1}{2}$, (i.e. the bug is highly infectious and infects 50% of the population), it may be sensible to place the median of the prior at around $(1 - 0.5 = 0.5)$. Extending this logic with the median at 0.5, 25%ile at 0.25 and 75%ile at 0.75. Plugging these into the MATCH elicitation tool, we get a scaled Beta Distribution with Params:

$a = 0.99$

$b = 1.00$

Let's try a more educated guess. According to [here](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases/articles/coronaviruscovid19latestinsights/infections), COVID-19 infection rate in England was around 3.21%. Let's say that (1 - 0.0321 = 0.9679) is the median, with 25%ile at 0.955 and 75%ile at 0.98. With this. we get a scaled Beta Distribution with Params:

$a = 82.58$

$b = 2.98$

This seems too heavily skewed. We can settle for something in between.

```{r}
n_25 = (0.25+0.955)/2
n_50 = (0.50+0.9679)/2
n_75 = (0.75+0.98)/2
print(n_25)
print(n_50)
print(n_75)
```

We use the tool [here](http://optics.eee.nottingham.ac.uk/match/uncertainty.php#) to estimate a and b for Beta distribution.
$a = 3.64$

$b = 1.46$

b. Use a “pseudo data” argument to select the Beta parameters (expressing current knowledge as if updating a uniform prior with “pseudo observations” as discussed in lectures)

Say we first have a uniform prior $\theta$, such that $\pi(\theta) \propto 1$. Then, our posterior $\pi(\theta|y) \sim Beta(s + 1 , n - s + 1)$ with $s = n\overline{y}$

We fabricate some data and say that $n$ = 7 and $\overline{y}$ = 0.85, such that s = 6. Now we have $\pi(\theta|y) \sim Beta(7 , 2)$ with $s = n\overline{y}$

c. Use the prior predictive distribution to justify your choice.

$p(y) = \int_{-\infty}^{\infty}p(y|\theta)\pi(\theta)d\theta$

Plugging in the prior predictive distribution,

$p(y) = \int_{-\infty}^{\infty}\theta^y(1-\theta)^{(1-y)}\frac{\tau(a+b)}{\tau(a)\tau(b)}\theta^{a-1}(1-\theta)^{b-1}d\theta$

$=\frac{\tau(a+b)}{\tau(a)\tau(b)}\int_{-\infty}^{\infty}\theta^{a+y-1}(1-\theta)^{b-y}d\theta$

$=\frac{\tau(a+b)}{\tau(a)\tau(b)}\int_{-\infty}^{\infty}\theta^{(a+y)-1}(1-\theta)^{(b-y+1)-1}d\theta$

$=\frac{\tau(a+b)}{\tau(a)\tau(b)}\frac{\tau(a+y)\tau(b-y+1)}{\tau(a+b+1)}\int_{-\infty}^{\infty}\frac{\tau(a+b+1)}{\tau(a+y)\tau(b-y+1)}\theta^{(a+y)-1}(1-\theta)^{(b-y+1)-1}d\theta$

$=\frac{\tau(a+b)}{\tau(a)\tau(b)}\frac{\tau(a+y)\tau(b-y+1)}{\tau(a+b+1)}$

Plugging in values of a and b,

$=\frac{\tau(9)}{\tau(7)\tau(2)}\frac{\tau(7+y)\tau(3-y)}{\tau(10)}$

Therefore, $p(y = 0) = \frac{1}{9}\frac{\tau(7)\tau(3)}{\tau(7)\tau(2)}$
$p(y = 0) = \frac{2}{9}$

And, $p(y = 1) = \frac{1}{9}\frac{\tau(8)\tau(2)}{\tau(7)\tau(2)}$
$p(y = 1) = \frac{7}{9}$ , which is consistent with the expectation of a $Beta(7,2)$ distribution, i.e. $\frac{a}{a+b}$

**3. Lucy attends 5 children's birthday parties in the run up to Percy’s and she judges that each one has the same probability of an attendee dropping out due to sickness. The number of expected attendees at each was 10, 50, 35, 25 and 40. There were 2, 8, 12, 6 and 8 absences respectively.**
What is your posterior distribution for $\theta$ (*which specific Beta distribution is it*)?

By Bayes,
$\pi(\theta|\boldsymbol{y}) \propto p(\boldsymbol{y}|\theta)\pi(\theta)$

$\pi(\theta|\boldsymbol{y}) \propto \pi(\theta)\prod_{i=1}^{n}p(y_i|\theta)$

$\pi(\theta|\boldsymbol{y}) \propto \theta^{a-1}(1-\theta)^{b-1}\prod_{i=1}^{n}\theta^{y_i}(1-\theta)^{1-y_i}$

$\pi(\theta|\boldsymbol{y}) \propto \theta^{a-1}(1-\theta)^{b-1}\theta^{\sum_{i=1}^{n}y_i}(1-\theta)^{n-\sum_{i=1}^{n}y_i}$

Let us substitute $s = \sum_{i=1}^{n}y_i = n\overline{y}$

$\pi(\theta|\boldsymbol{y}) \propto \theta^{a-1}(1-\theta)^{b-1}\theta^{s}(1-\theta)^{n-s}$


$\pi(\theta|\boldsymbol{y}) \propto \theta^{a+s-1}(1-\theta)^{n-s+b-1}$

Hence, $\pi(\theta|\boldsymbol{y})$ follows a Beta distribution

$\theta|\boldsymbol{y} \sim Beta(a+s , n-s+b)$

Plugging in our determined values for a and b, we get
$\theta|\boldsymbol{y} \sim Beta(3.64+s , n-s+1.46)$

We know from our data that

```{r}
y_trials = c(10, 50, 35, 25, 40)
y_failures = c(2, 8, 12, 6, 8)
y_successes = y_trials - y_failures
print(sum(y_trials)) # Value for n
print(sum(y_successes)) # Value for s
print(sum(y_successes)/sum(y_trials)) # Value for y_bar
```

Plugging in values for s and n, we get
$\theta|\boldsymbol{y} \sim Beta(3.64+s , n-s+1.46)$,
We know the values of n as 160 and s as 124 

$\theta|\boldsymbol{y} \sim Beta(127.64, 37.46)$


**4.  Plot your prior and posterior on the same figure and give critical comment on the influence of the data on your prior beliefs.**
```{r}
a = 3.64
b = 1.46

plot_beta_prior = function(successes , trials , a , b , n){
  
  binary_data <- tibble(x=seq(from = 0 , to = 1 , len = n)) %>%
    mutate(prior = dbeta(x , a , b)) %>%
    mutate(posterior = dbeta(x , successes + a , trials - successes + b))%>%
    pivot_longer(cols = c(2,3) , names_to = 'distribution' , values_to = 'density')
  
  ggplot(binary_data) +
    geom_line(aes(x = x , y = density , col = distribution)) +
    geom_vline(xintercept = successes/trials)
}

# Prior has an effect
plot_beta_prior(sum(y_successes) , sum(y_trials) , a , b , 100)
```
We see that the data has significantly changed out prior beliefs.

*Critical Comments*
Even though the peak (mode) of the prior was closer to $1.0$ than that of the posterior or even the average successes rate observed by Lucy, it was still relatively spread out over the range $(0,1)$. On the other hand, the posterior has condensed the probabilities around $0.75$ to $0.8$, which happens to be very close to the average number of success rate observed by Lucy $\frac{124}{160} = 0.775$. Incorporating data from the observations has thus allowed us to have a much closer approximation of what the 'true' $\theta$ parameter might be.

**5. Using only uniform random numbers, estimate the posterior predictive probability that more than 50 people attend Percy’s birthday party using Monte Carlo, ensuring that the error on your estimate is bounded above by 0.01.**
You must:

- Write down the probability you require as the appropriate integral.
- State clearly how your uniform samples are converted to samples from the right distributions.
- Only use sampling algorithms shown during the course and when more than one sampler is available, you must choose the most efficient.
- Report the Monte Carlo Error of your estimate.
- Include the code from your sampler in your report.
- Whilst you may not use any inbuilt R samplers, you may use their inbuilt density/distribution functions (so rbinom cannot be used but dbinom and pbinom can be).
